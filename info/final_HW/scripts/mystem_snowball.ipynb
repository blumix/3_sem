{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import tqdm\n",
    "import sys\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "import logging\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = Mystem()\n",
    "stemmer = SnowballStemmer('russian', ignore_stopwords=True)\n",
    "stop = stopwords.words('russian')\n",
    "TRY_NAME = \"WORDS_TITLE_BODY\"\n",
    "stop.extend ([\"\\n\", \" \"])\n",
    "\n",
    "dct = Dictionary(prune_at=None)\n",
    "\n",
    "def apply_to_str (string):\n",
    "    no_stops = filter (lambda x: x not in stop, lemmatizer.lemmatize (string))\n",
    "    return map(lambda x: stemmer.stem(x), no_stops)\n",
    "\n",
    "with open (\"../data/docs.tsv\") as fin:    \n",
    "    for doc in tqdm.tqdm (fin, total=582167):\n",
    "        doc = doc.decode (\"utf-8\").lower ().strip ().split(\"\\t\")\n",
    "        dct.add_documents ([apply_to_str (' '.join (doc[1:]))], prune_at=None)\n",
    "\n",
    "dct.save (\"../result/{}/dict.dct\".format (TRY_NAME))\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def doc_reader ():\n",
    "    with open (\"../data/docs.tsv\") as fin:\n",
    "        for i, doc in enumerate (fin):\n",
    "            doc = doc.decode (\"utf-8\").lower ().strip ().split(\"\\t\")[1:]\n",
    "            yield dct.doc2bow(apply_to_str (' '.join (doc)))\n",
    "            \n",
    "corpora.MmCorpus.serialize('../result/{}/corpus.mm'.format (TRY_NAME), doc_reader ())\n",
    "\n",
    "import gensim as gs\n",
    "import math\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "dct_title = gs.corpora.Dictionary.load (\"../result/{}/dict.dct\".format (TRY_NAME))\n",
    "\n",
    "corpus = gs.corpora.MmCorpus('../result/{}/corpus.mm'.format (TRY_NAME))\n",
    "\n",
    "PARAM_K1 = 1.5\n",
    "PARAM_B = 0.75\n",
    "EPSILON = 0.25\n",
    "\n",
    "corp_size = 0\n",
    "corp_size_words = 0\n",
    "for doc in tqdm.tqdm (corpus):\n",
    "    corp_size_words += sum (dict (doc).itervalues ())\n",
    "    corp_size += 1\n",
    "\n",
    "avgdl = float (corp_size_words) / corp_size\n",
    "\n",
    "idfs = {}\n",
    "\n",
    "summ_idf = 0\n",
    "\n",
    "for word_id, freq in dct_title.dfs.iteritems ():\n",
    "    idfs[word_id] = math.log(corp_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "    summ_idf += idfs[word_id]\n",
    "\n",
    "average_idf = float (summ_idf) / len (dct_title.dfs)\n",
    "\n",
    "for id in idfs.iterkeys ():\n",
    "    idfs[id] = idfs[id] if idfs[id] >= 0 else EPSILON * average_idf\n",
    "\n",
    "q_ids = []\n",
    "queries = []\n",
    "with open (\"../data/queries.tsv\") as fin:\n",
    "    for q in fin:\n",
    "        q = q.strip ().decode (\"utf-8\").lower ().split (\"\\t\")\n",
    "        q_ids.append (int (q[0]))\n",
    "        queries.append (dict (dct_title.doc2bow (apply_to_str (q[1]))))\n",
    "\n",
    "q_size = len (queries)\n",
    "\n",
    "result = np.zeros ((q_size, corp_size))\n",
    "\n",
    "for doc_i, doc in tqdm.tqdm (enumerate (corpus), total=582167):\n",
    "    doc = dict (doc)\n",
    "    doc_keys = set (doc)\n",
    "    doc_len = sum (doc.itervalues ())\n",
    "    for q_i, q in enumerate (queries):\n",
    "        score = 0\n",
    "        for word in set (q.keys ()) & doc_keys:\n",
    "            idf = idfs[word]\n",
    "            score += (idf * doc[word] * (PARAM_K1 + 1)\n",
    "                      / (doc[word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * doc_len / avgdl)))\n",
    "        result[q_i, doc_i] = score\n",
    "\n",
    "with open (\"../result/result_{}.csv\".format (TRY_NAME), \"w\") as fout:\n",
    "    fout.write (\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "    for q_num, qid in tqdm.tqdm (enumerate (q_ids)):\n",
    "        for doc in np.argsort (result[q_num, :])[-5:][::-1]:\n",
    "            fout.write (\"{},{}\\n\".format (qid, doc))\n",
    "\n",
    "np.save (open (\"../result/{}/ranking.npy\".format (TRY_NAME), \"wb\"), result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import tqdm\n",
    "import sys\n",
    "from gensim import corpora\n",
    "from gensim import corpora, models, similarities\n",
    "import numpy as np\n",
    "import logging\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def apply_to_str (string, lemmatizer, stemmer, stop):\n",
    "    no_stops = filter (lambda x: x not in stop, lemmatizer.lemmatize (string))\n",
    "    return map(lambda x: stemmer.stem(x), no_stops)\n",
    "\n",
    "def process_file (fname):\n",
    "    lemmatizer = Mystem()\n",
    "    stemmer = SnowballStemmer('russian', ignore_stopwords=True)\n",
    "    stop = stopwords.words('russian')\n",
    "    stop.extend ([\"\\n\", \" \"])\n",
    "    with open ('../data/' + fname) as f:\n",
    "        with open ('../temp/' + fname + \"_result\", \"w\") as fout:\n",
    "            for i, line in enumerate (f):\n",
    "                splited = line.decode (\"utf-8\").lower ().strip ().split (\"\\t\")\n",
    "                res_str = splited[0] + \"\\t\"\n",
    "                for s in splited[1:]:\n",
    "                    res_str += ' '.join (apply_to_str (s, lemmatizer, stemmer, stop)) + \"\\t\"\n",
    "                res_str += '\\n'\n",
    "                fout.write (res_str.encode ('utf-8'))\n",
    "                if i % 1000 == 0:\n",
    "                    print fname + \" at \" + str (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['xaa','xab','xac','xad','xae','xaf','xag','xah','xai','xaj','xak','xal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = Pool (12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.map (process_file, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
