{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import magic\n",
    "import sys \n",
    "import tqdm\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "patt = re.compile (r'[\\W^\\d]+', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urls (f_name):\n",
    "    urls = {}\n",
    "    with open(f_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    for url in content:\n",
    "        split = url.split ('\\t')\n",
    "        urls[int(split[0])] = split[1].decode ('utf-8')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries (f_name):\n",
    "    urls = {}\n",
    "    patt = re.compile (r'[\\W^\\d]+', flags=re.UNICODE)\n",
    "    with open(f_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    for url in content:\n",
    "        split = url.split ('\\t')\n",
    "        urls[int(split[0])] = set ([word for word in set (patt.split (split[1].decode ('utf-8').lower ())) if word not in stopwords.words('russian')])\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = read_urls (\"urls.numerate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = read_queries (\"queries.numerate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc (doc_name):\n",
    "    with open(doc_name) as f:\n",
    "        name = f.readline ()\n",
    "        html = f.read()\n",
    "        m = magic.Magic(mime_encoding=True)\n",
    "        encoding = m.from_buffer(html)\n",
    "        try:\n",
    "            html = html.decode (encoding)\n",
    "        except:\n",
    "#             sys.stderr.write (\"{}, unknown encoding. skipping\".format (doc_name))\n",
    "            return None\n",
    "            \n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return name, text.lower ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_idf (queries):\n",
    "    mypath = 'content/'\n",
    "    dirs = listdir(mypath)\n",
    "    onlyfiles = []\n",
    "    len_of_file = []\n",
    "    for d in dirs:\n",
    "        onlyfiles.extend ([mypath + d + '/' + f for f in listdir(mypath + d) ])\n",
    "        print onlyfiles\n",
    "    docs_for_querie = defaultdict (set)\n",
    "    t = time.time ()\n",
    "    words = Counter ()\n",
    "    num = len (onlyfiles)\n",
    "    skipped = []\n",
    "    s_num = 0\n",
    "    for i, f in enumerate(onlyfiles):\n",
    "        doc = read_doc (f)\n",
    "        if doc is None:\n",
    "            skipped.append (f)\n",
    "            s_num += 1\n",
    "            continue\n",
    "        word_in_doc = patt.split (doc[1])\n",
    "        filtered_words = [word for word in set (word_in_doc) if word not in stopwords.words('russian')]\n",
    "        len_of_file.append (len (word_in_doc))\n",
    "\n",
    "        words.update (filtered_words)\n",
    "#         for q in queries.iteritems ():\n",
    "#             if q[1].issubset(filtered_words):\n",
    "#                 docs_for_querie[q[0]].add (f)\n",
    "        sys.stdout.write (\"\\r {}% scanned. {} skipped.                  \".format (round (i * 100. / num,3), s_num))\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "    sys.stdout.write (\"\\r Done. {} skipped. {} Time Elapsed.            \".format (s_num, time.time () - t))\n",
    "    avrg_len = np.mean (len_of_file)\n",
    "    return words, skipped, docs_for_querie, avrg_len, len (onlyfiles) - skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done. 1 skipped. 9.47841405869 Time Elapsed.            "
     ]
    }
   ],
   "source": [
    "words, skipped, doc_f_q, avrg_len, num_of_docs = count_idf (queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_idf (idf, num_of_docs):\n",
    "    for d in idf.iteritems ():\n",
    "        res = np.log ((num_of_docs - d[1] + 0.5) / (d[1] + 0.5))\n",
    "        if res > 0:\n",
    "            idf[d[0]] = res\n",
    "        else:\n",
    "            idf[d[0]] = 0\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25 (query, idf, doc, k, b,avrg_len):\n",
    "    words = Counter (doc)\n",
    "    bm25 = 0\n",
    "    len_D = len (doc)\n",
    "    for q in query:\n",
    "        bm25 += idf[q] * words[q] * (k + 1) / ( words[q] + k * (1 - b + b*len_D / avrg_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bm25 (queries, words, skipped, doc_f_q, avrg_len, b=0.75, k=2.0):\n",
    "    mypath = 'content/'\n",
    "    dirs = listdir(mypath)\n",
    "    onlyfiles = []\n",
    "    for d in dirs:\n",
    "        onlyfiles.extend ([mypath + d + '/' + f for f in listdir(mypath + d) ])\n",
    "        print onlyfiles\n",
    "    docs_for_querie = defaultdict (set)\n",
    "    t = time.time ()\n",
    "    num = len (onlyfiles)\n",
    "    skipped = []\n",
    "    s_num = 0\n",
    "    for i, f in enumerate(onlyfiles):\n",
    "        doc = read_doc (f)\n",
    "        if doc is None:\n",
    "            skipped.append (f)\n",
    "            s_num += 1\n",
    "            continue\n",
    "        filtered_words = [word for word in set (patt.split (doc[1])) if word not in stopwords.words('russian')]\n",
    "        for q in queries.iteritems ():\n",
    "            if q[1].issubset(filtered_words):\n",
    "                docs_for_querie[q[0]].add (f)\n",
    "        sys.stdout.write (\"\\r {}% scanned. {} skipped.                  \".format (round (i * 100. / num,3), s_num))\n",
    "        if i > 10:\n",
    "            break\n",
    "\n",
    "    sys.stdout.write (\"\\r Done. {} skipped. {} Time Elapsed.            \".format (s_num, time.time () - t))\n",
    "\n",
    "    return words, skipped, docs_for_querie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
