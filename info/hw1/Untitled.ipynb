{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   \n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import magic\n",
    "import sys \n",
    "import tqdm\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import heapq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patt = re.compile (r'[\\W^\\d]+', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_urls (f_name):\n",
    "    urls = {}\n",
    "    with open(f_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    for url in content:\n",
    "        split = url.split ('\\t')\n",
    "        urls[int(split[0])] = split[1].decode ('utf-8')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries (f_name):\n",
    "    urls = {}\n",
    "    patt = re.compile (r'[\\W^\\d]+', flags=re.UNICODE)\n",
    "    with open(f_name) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    for url in content:\n",
    "        split = url.split ('\\t')\n",
    "        urls[int(split[0])] = set ([word for word in set (patt.split (split[1].decode ('utf-8').lower ())) if word not in stopwords.words('russian')])\n",
    "        if u'' in urls[int(split[0])]:\n",
    "            urls[int(split[0])].remove (u'')\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = read_urls (\"urls.numerate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = read_queries (\"queries.numerate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc (doc_name):\n",
    "    with open(doc_name) as f:\n",
    "        name = f.readline ()\n",
    "        html = f.read()\n",
    "        m = magic.Magic(mime_encoding=True)\n",
    "        encoding = m.from_buffer(html)\n",
    "        try:\n",
    "            html = html.decode (encoding)\n",
    "        except:\n",
    "#             sys.stderr.write (\"{}, unknown encoding. skipping\".format (doc_name))\n",
    "            return None\n",
    "            \n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return name, text.lower ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_idf (queries):\n",
    "    mypath = 'content/'\n",
    "    dirs = listdir(mypath)\n",
    "    onlyfiles = []\n",
    "    len_of_file = []\n",
    "    for d in dirs:\n",
    "        onlyfiles.extend ([mypath + d + '/' + f for f in listdir(mypath + d) ])\n",
    "        print onlyfiles\n",
    "    docs_for_querie = defaultdict (set)\n",
    "    t = time.time ()\n",
    "    words = Counter ()\n",
    "    num = len (onlyfiles)\n",
    "    skipped = []\n",
    "    s_num = 0\n",
    "    for i, f in enumerate(onlyfiles):\n",
    "        doc = read_doc (f)\n",
    "        if doc is None:\n",
    "            skipped.append (f)\n",
    "            s_num += 1\n",
    "            continue\n",
    "        word_in_doc = patt.split (doc[1])\n",
    "        filtered_words = [word for word in set (word_in_doc) if word not in stopwords.words('russian')]\n",
    "        len_of_file.append (len (word_in_doc))\n",
    "\n",
    "        words.update (filtered_words)\n",
    "#         for q in queries.iteritems ():\n",
    "#             if q[1].issubset(filtered_words):\n",
    "#                 docs_for_querie[q[0]].add (f)\n",
    "        sys.stdout.write (\"\\r {}% scanned. {} skipped.                  \".format (round (i * 100. / num,3), s_num))\n",
    "    sys.stdout.write (\"\\r Done. {} skipped. {} Time Elapsed.            \".format (s_num, time.time () - t))\n",
    "    avrg_len = np.mean (len_of_file)\n",
    "    return words, skipped, docs_for_querie, avrg_len, len (onlyfiles) - len (skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/Users/mikhailbelozerov/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/mikhailbelozerov/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done. 873 skipped. 39596.914866 Time Elapsed.            "
     ]
    }
   ],
   "source": [
    "words, skipped, doc_f_q, avrg_len, num_of_docs = count_idf (queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15880"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pop (u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_idf (words, num_of_docs):\n",
    "    idf = {}\n",
    "    for d in words.iteritems ():\n",
    "        res = np.log ((num_of_docs - d[1] + 0.5) / (d[1] + 0.5))\n",
    "        if res > 0:\n",
    "            idf[d[0]] = res\n",
    "        else:\n",
    "            idf[d[0]] = 0\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25 (query, idf, doc, k, b, avrg_len):\n",
    "    words = Counter (doc)\n",
    "    bm25 = 0\n",
    "    len_D = len (doc)\n",
    "    for q in query:\n",
    "        bm25 += idf[q] * idf[q] * (k + 1) / ( idf[q] + k * (1 - b + b*len_D / avrg_len))\n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bm25 (queries, idf, avrg_len, b=0.75, k=2.0):\n",
    "    mypath = 'content/'\n",
    "    dirs = listdir(mypath)\n",
    "    onlyfiles = []\n",
    "    for d in dirs:\n",
    "        onlyfiles.extend ([mypath + d + '/' + f for f in listdir(mypath + d) ])\n",
    "    docs_for_querie = defaultdict (list)\n",
    "    t = time.time ()\n",
    "    num = len (onlyfiles)\n",
    "    skipped = []\n",
    "    s_num = 0\n",
    "    for i, f in enumerate(onlyfiles):\n",
    "        doc = read_doc (f)\n",
    "        if doc is None:\n",
    "            skipped.append (f)\n",
    "            s_num += 1\n",
    "            continue\n",
    "        doc_name = doc[0]\n",
    "        doc = patt.split (doc[1])\n",
    "        filtered_words = [word for word in set (doc) if word not in stopwords.words('russian')]\n",
    "        for q in queries.iteritems ():\n",
    "            if q[1].issubset(filtered_words):\n",
    "                bm = get_bm25 (q[1], idf, doc,k,b,avrg_len)\n",
    "                if len (docs_for_querie[q[0]]) == 10:\n",
    "                    heapq.heappushpop (docs_for_querie[q[0]], (bm, doc_name))\n",
    "                else:\n",
    "                    heapq.heappush (docs_for_querie[q[0]], (bm, doc_name))\n",
    "        sys.stdout.write (\"\\r {}% scanned. {} skipped.                  \".format (round (i * 100. / num,3), s_num))\n",
    "\n",
    "    sys.stdout.write (\"\\r Done. {} skipped. {} Time Elapsed.            \".format (s_num, time.time () - t))\n",
    "\n",
    "    return docs_for_querie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = update_idf (words, num_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13.696% scanned. 108 skipped.                  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f7018e38d3c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_bm25\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-82cc798ac345>\u001b[0m in \u001b[0;36mcount_bm25\u001b[0;34m(queries, idf, avrg_len, b, k)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0ms_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monlyfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_doc\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mskipped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8c781099330d>\u001b[0m in \u001b[0;36mread_doc\u001b[0;34m(doc_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# kill all script and style elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikhailbelozerov/anaconda2/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikhailbelozerov/anaconda2/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mikhailbelozerov/anaconda2/lib/python2.7/site-packages/bs4/builder/_lxml.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParserError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed (src/lxml/lxml.etree.c:114563)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parser.pxi\u001b[0m in \u001b[0;36mlxml.etree._FeedParser.feed (src/lxml/lxml.etree.c:114436)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult (src/lxml/lxml.etree.c:131229)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._TargetParserContext._handleParseResult (src/lxml/lxml.etree.c:131025)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/lxml.etree.pyx\u001b[0m in \u001b[0;36mlxml.etree._ExceptionContext._raise_if_stored (src/lxml/lxml.etree.c:12167)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._handleSaxTargetStartNoNs (src/lxml/lxml.etree.c:123503)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/saxparser.pxi\u001b[0m in \u001b[0;36mlxml.etree._callTargetSaxStart (src/lxml/lxml.etree.c:123700)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/lxml/parsertarget.pxi\u001b[0m in \u001b[0;36mlxml.etree._PythonSaxParserTarget._handleSaxStart (src/lxml/lxml.etree.c:130022)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/mikhailbelozerov/anaconda2/lib/python2.7/site-packages/bs4/builder/_lxml.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, name, attrs, nsmap)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# turn then into NamespacedAttribute objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mnew_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getNsTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnamespace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs = count_bm25 (queries, idf, avrg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
